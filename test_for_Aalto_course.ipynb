{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlxHT/cTRjLYm/AQptEYp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qahaidari/Deep-learning-for-classification-and-regression/blob/master/test_for_Aalto_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aUN8yjO37L3X",
        "outputId": "1a2217ac-0047-4748-ccf7-2c136a5b2205"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/lsp_dataset.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/lsp_dataset')"
      ],
      "metadata": {
        "id": "nL8kJ8jj6Mzm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REgLlzSR557k"
      },
      "outputs": [],
      "source": [
        "# 1. Data loading for regression: Using custom dataset for LSP\n",
        "# Link for downloading LSP dataset: http://sam.johnson.io/research/lsp.html \n",
        "# Visualization without resizing images\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from os.path import dirname, normpath, normcase, join as pjoin\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from skimage import io\n",
        "import os\n",
        "\n",
        "class CustomLSP(Dataset):\n",
        "  \n",
        "  def __init__(self, data_dir, root_dir):\n",
        "    joints_dir = pjoin(data_dir, 'joints.mat')\n",
        "    self.images_folder = pjoin(data_dir, 'images')\n",
        "    mat_contents = sio.loadmat(joints_dir) # labels are in the form of MATLAB mat files\n",
        "    self.joints = mat_contents['joints']\n",
        "    self.images = sorted(os.listdir(self.images_folder)) # sorted list of images\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    im = pjoin(self.images_folder, self.images[index])\n",
        "    im = plt.imread(im) # numpy array of the image\n",
        "    joints = self.joints[:,:,index]\n",
        "    return im, joints\n",
        "  \n",
        "  def __len__(self):\n",
        "    self.len = print(im.shape)\n",
        "    return self.len\n",
        "\n",
        "data_dir = pjoin('./lsp_dataset') # LSP dataset should already exist in the main root of the project directory\n",
        "data = CustomLSP(data_dir, root_dir='/')\n",
        "\n",
        "################### Visualization ############################\n",
        "num_of_images = 3\n",
        "for index in range(0, num_of_images):\n",
        "  images, joints = data[index]\n",
        "  print('sample #{} has shape {}'.format(index, images.shape))\n",
        "  f = plt.figure(figsize=(8,8))\n",
        "  ax = f.add_subplot(1, num_of_images, index+1)\n",
        "  ax.axis('off')\n",
        "  ax.imshow(images)\n",
        "  ax.plot([joints[0,0],joints[0,1],joints[0,2]],[joints[1,0],joints[1,1],joints[1,2]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,3],joints[0,4],joints[0,5]],[joints[1,3],joints[1,4],joints[1,5]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,6],joints[0,7],joints[0,8]],[joints[1,6],joints[1,7],joints[1,8]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,9],joints[0,10],joints[0,11]],[joints[1,9],joints[1,10],joints[1,11]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,12],joints[0,13]],[joints[1,12],joints[1,13]],marker = 'o', c='r', zorder=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data loading for regression: Using custom data loader for LSP with variable batch size dataloader and splitting dataset to train and test sets\n",
        "# Link for downloading LSP dataset: http://sam.johnson.io/research/lsp.html\n",
        "# Visualization after resizing images\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from os.path import dirname, normpath, normcase, join as pjoin\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from skimage import io\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "Batch_size = 3; # samples in one batch\n",
        "train_size = 80; # 80% of the data\n",
        "test_size = 100 - train_size; # 20% of the data \n",
        "\n",
        "class CustomLSP(Dataset):\n",
        "  \n",
        "  def __init__(self, data_dir, train_size, train):\n",
        "    joints_dir = pjoin(data_dir, 'joints.mat')\n",
        "    images_folder = pjoin(data_dir, 'images')\n",
        "    mat_contents = sio.loadmat(joints_dir)\n",
        "    joints = mat_contents['joints']\n",
        "    images = sorted(os.listdir(images_folder)) # sorted list of images\n",
        "    self.train = train\n",
        "\n",
        "    if self.train == True:\n",
        "      self.train_len = train_size * len(images) / 100 # this is train_size * 2000 / 100\n",
        "      self.train_image_set = []\n",
        "      self.train_joint_set = []\n",
        "      for i in range(0,int(self.train_len)):\n",
        "        \n",
        "        # resizing images to equal sizes and modifying joints accordingly\n",
        "        im = plt.imread(pjoin(images_folder,images[i]))\n",
        "        h, w = im.shape[:2]\n",
        "        im = cv2.resize(im, (256, 256)) \n",
        "        im = torch.from_numpy(im)\n",
        "        self.train_image_set.append(im)\n",
        "        joints_i = torch.from_numpy(joints[:2,:,i]) # ignoring the visibility term in joints\n",
        "        joints_i[0,:] = torch.mul(joints_i[0,:], 256/w) # modifying joints\n",
        "        joints_i[1,:] = torch.mul(joints_i[1,:], 256/h) # modifying joints\n",
        "        self.train_joint_set.append(joints_i)\n",
        "        \n",
        "    if self.train == False:\n",
        "      self.train_len = train_size * len(images) / 100 # this is train_size * 2000 / 100\n",
        "      self.test_len = (100 - train_size) * len(images) / 100\n",
        "      self.test_image_set = []\n",
        "      self.test_joint_set = []\n",
        "      for i in range(int(self.train_len),len(images)):\n",
        "\n",
        "        # resizing images to equal sizes and modifying joints accordingly\n",
        "        im = plt.imread(pjoin(images_folder,images[i]))\n",
        "        h , w = im.shape[:2]\n",
        "        im = cv2.resize(im, (256, 256)) \n",
        "        im = torch.from_numpy(im)\n",
        "        self.test_image_set.append(im)\n",
        "        joints_i = torch.from_numpy(joints[:2,:,i]) # ignoring the visibility term in joints\n",
        "        joints_i[0,:] = torch.mul(joints_i[0,:], 256/w) # modifying joints\n",
        "        joints_i[1,:] = torch.mul(joints_i[1,:], 256/h) # modifying joints\n",
        "        self.test_joint_set.append(joints_i)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if self.train == True:\n",
        "      train_images = self.train_image_set[index]\n",
        "      train_joints = self.train_joint_set[index]\n",
        "      return train_images, train_joints\n",
        "    if self.train == False:\n",
        "      test_images = self.test_image_set[index]\n",
        "      test_joints = self.test_joint_set[index]\n",
        "      return test_images, test_joints\n",
        "  \n",
        "  def __len__(self):\n",
        "    if self.train == True:\n",
        "      return int(self.train_len)\n",
        "    if self.train == False:\n",
        "      return int(self.test_len)\n",
        "\n",
        "data_dir = pjoin('./lsp_dataset') # LSP dataset should already exist in the main root of the project directory\n",
        "train_set = CustomLSP(data_dir, train_size=80, train=True)\n",
        "test_set = CustomLSP(data_dir, train_size=80, train=False)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, Batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, Batch_size, shuffle=True)\n",
        "dataiter = iter(train_loader)\n",
        "train_images, train_joints = dataiter.next()\n",
        "################### Visualization ############################\n",
        "num_of_images = Batch_size\n",
        "for index in range(0, num_of_images):\n",
        "  image = train_images[index]\n",
        "  joints = train_joints[index]\n",
        "  print('sample #{} has shape {}'.format(index, image.shape))\n",
        "  f = plt.figure(figsize=(10,10))\n",
        "  ax = f.add_subplot(1, num_of_images, index+1)\n",
        "  ax.axis('off')\n",
        "  ax.imshow(image)\n",
        "  ax.plot([joints[0,0],joints[0,1],joints[0,2]],[joints[1,0],joints[1,1],joints[1,2]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,3],joints[0,4],joints[0,5]],[joints[1,3],joints[1,4],joints[1,5]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,6],joints[0,7],joints[0,8]],[joints[1,6],joints[1,7],joints[1,8]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,9],joints[0,10],joints[0,11]],[joints[1,9],joints[1,10],joints[1,11]],marker = 'o', c='r', zorder=1)\n",
        "  ax.plot([joints[0,12],joints[0,13]],[joints[1,12],joints[1,13]],marker = 'o', c='r', zorder=1)"
      ],
      "metadata": {
        "id": "rAKHgrbH71WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model Training for Regression: LSP\n",
        "# Link for downloading LSP dataset: http://sam.johnson.io/research/lsp.html\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from os.path import dirname, normpath, normcase, join as pjoin\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from skimage import io\n",
        "import os\n",
        "import math\n",
        "from torch.nn.functional import cross_entropy\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_shape[0], 6, (5, 5))\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 12, (5, 5))\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(12, 12, (5, 5))\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2, stride=1)\n",
        "        self.linear1 = nn.Linear(56*56*12, 120)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(120, 60)\n",
        "        self.act5 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(60,30) \n",
        "        self.act6 = nn.ReLU()\n",
        "        self.linear4 = nn.Linear(30,28)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.act1(self.conv1(x.float())))\n",
        "        x = self.pool2(self.act2(self.conv2(x)))\n",
        "        x = self.pool3(self.act3(self.conv3(x)))\n",
        "        x = x.view(-1, 56*56*12)\n",
        "        x = self.act4(self.linear1(x))\n",
        "        x = self.act5(self.linear2(x))\n",
        "        x = self.act6(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "class CustomLSP(Dataset):\n",
        "  \n",
        "  def __init__(self, data_dir, train_size, train):\n",
        "    joints_dir = pjoin(data_dir, 'joints.mat')\n",
        "    images_folder = pjoin(data_dir, 'images')\n",
        "    mat_contents = sio.loadmat(joints_dir)\n",
        "    joints = mat_contents['joints']\n",
        "    images = sorted(os.listdir(images_folder)) # sorted list of images\n",
        "    self.train = train\n",
        "\n",
        "    if self.train == True:\n",
        "      self.train_len = train_size * len(images) / 100 # this is train_size * 2000 / 100\n",
        "      self.train_image_set = []\n",
        "      self.train_joint_set = []\n",
        "      for i in range(0,int(self.train_len)):\n",
        "        \n",
        "        # resizing images to equal sizes and modifying joints accordingly\n",
        "        im = plt.imread(pjoin(images_folder,images[i]))\n",
        "        h, w = im.shape[:2]\n",
        "        im = cv2.resize(im, (256, 256)) \n",
        "        im = torch.from_numpy(im)\n",
        "\n",
        "        im = im.view(3,1,256*256).view(3,256,256)\n",
        "\n",
        "        self.train_image_set.append(im)\n",
        "        joints_i = torch.from_numpy(joints[:2,:,i]) # ignoring the visibility term in joints\n",
        "        joints_i[0,:] = torch.mul(joints_i[0,:], 256/w) # modifying joints\n",
        "        joints_i[1,:] = torch.mul(joints_i[1,:], 256/h) # modifying joints\n",
        "        joints_i = torch.reshape(joints_i,(1,28)) # joints reshaped so that later joints and predictions have same shape for loss calculation\n",
        "        self.train_joint_set.append(joints_i)  \n",
        "        \n",
        "    if self.train == False:\n",
        "      self.train_len = train_size * len(images) / 100 # this is train_size * 2000 / 100\n",
        "      self.test_len = (100 - train_size) * len(images) / 100\n",
        "      self.test_image_set = []\n",
        "      self.test_joint_set = []\n",
        "      for i in range(int(self.train_len),len(images)):\n",
        "\n",
        "        # resizing images to equal sizes and modifying joints accordingly\n",
        "        im = plt.imread(pjoin(images_folder,images[i]))\n",
        "        h, w = im.shape[:2]\n",
        "        im = cv2.resize(im, (256, 256)) \n",
        "        im = torch.from_numpy(im)\n",
        "        \n",
        "        im = im.view(3,1,256*256).view(3,256,256)\n",
        "\n",
        "        self.test_image_set.append(im)\n",
        "        joints_i = torch.from_numpy(joints[:2,:,i]) # ignoring the visibility term in joints\n",
        "        joints_i[0,:] = torch.mul(joints_i[0,:], 256/w) # modifying joints\n",
        "        joints_i[1,:] = torch.mul(joints_i[1,:], 256/h) # modifying joints\n",
        "        joints_i = torch.reshape(joints_i,(1,28)) # joints reshaped so that later joints and predictions have same shape for loss calculation\n",
        "        self.test_joint_set.append(joints_i)\n",
        "        \n",
        "  def __getitem__(self, index):\n",
        "    if self.train == True:\n",
        "      train_images = self.train_image_set[index]\n",
        "      train_joints = self.train_joint_set[index]\n",
        "      return train_images, train_joints\n",
        "    if self.train == False:\n",
        "      test_images = self.test_image_set[index]\n",
        "      test_joints = self.test_joint_set[index]\n",
        "      return test_images, test_joints\n",
        "  \n",
        "  def __len__(self):\n",
        "    if self.train == True:\n",
        "      return int(self.train_len)\n",
        "    if self.train == False:\n",
        "      return int(self.test_len)\n",
        "\n",
        "NUM_EPOCHS = 10 # number of iterations\n",
        "BATCH_SIZE = 64 # samples in one batch\n",
        "LR = 0.001 # learning rate\n",
        "train_size = 80 # 80% of the data\n",
        "test_size = 100 - train_size # 20% of the data \n",
        "\n",
        "def main():\n",
        "  data_dir = pjoin('./lsp_dataset') # LSP dataset already exists in the main root of the project directory\n",
        "  train_set = CustomLSP(data_dir, train_size=80, train=True)\n",
        "  test_set = CustomLSP(data_dir, train_size=80, train=False)\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  # instantiate the model\n",
        "  model = Network((3,256, 256))\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    for mode, data in [(\"train\", train_loader), (\"test\", test_loader)]:\n",
        "  \n",
        "      runningLoss = 0.\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for step, (images, joints) in enumerate(data): # looping over each batch data\n",
        "        \n",
        "        predictions = model.forward(images)\n",
        "        total += images.shape[0]\n",
        "\n",
        "        error = joints - predictions\n",
        "        loss = error.pow(2).mean()\n",
        "    \n",
        "        runningLoss += loss.item() * images.shape[0]\n",
        "        \n",
        "        if mode == \"train\":\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      (train_losses if mode == \"train\" else test_losses).append(runningLoss / total)\n",
        "      print(\"epochs: {} | {} | total loss: {}\".format(epoch,mode,runningLoss/total))\n",
        "\n",
        "  with open('/LSP_train_test_losses.pickle', \"wb\") as f: # file = 'LSP_train_test_losses.pickle'\n",
        "    pickle.dump((train_losses, test_losses), f)\n",
        "          \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "######################## Plots #################################################\n",
        "with open(\"/LSP_train_test_losses.pickle\", \"rb\") as f:\n",
        "        train_losses, test_losses = pickle.load(f)\n",
        "\n",
        "plt.plot(np.arange(0, len(train_losses)), train_losses, label= \"train loss\", color=\"blue\")\n",
        "plt.plot(np.arange(0, len(train_losses)), test_losses, label= \"test loss\", color=\"green\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GCLeBlHj7_NT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}